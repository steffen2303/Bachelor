\relax 
\providecommand*\new@tpo@label[2]{}
\catcode `"\active 
\select@language{ngerman}
\@writefile{toc}{\select@language{ngerman}}
\@writefile{lof}{\select@language{ngerman}}
\@writefile{lot}{\select@language{ngerman}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Einf{\"u}hrung}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:introduction}{{1}{1}}
\citation{bib:neuron}
\citation{bib:neuron}
\citation{bib:aneuron}
\citation{bib:aneuron}
\citation{bib:number}
\citation{bib:mlp}
\citation{bib:mlp}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Grundlagen}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:grundlagen}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Neuronale Netze}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Schematische Darstellung eines biologischen Neuron \cite  {bib:neuron}}}{2}}
\newlabel{img:neuron}{{2.1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Schematische Darstellung einer M\IeC {\"o}glichkeit, ein k\IeC {\"u}nstliches Neuron zu simulieren \cite  {bib:aneuron}}}{3}}
\newlabel{img:aneuron}{{2.2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Ein Multilayer Perceptron mit 2 Inputneuronen(gr\IeC {\"u}n), einem Hidden Layer mit 5 Neuronen(blau) und einem einzelnen Outputneuron(gelb). \cite  {bib:mlp}}}{3}}
\newlabel{img:mlp}{{2.3}{3}}
\newlabel{eq:act}{{2.1}{4}}
\newlabel{eq:err}{{2.2}{4}}
\newlabel{eq:err}{{2.3}{4}}
\citation{bib:nn}
\citation{bib:bp}
\citation{bib:rnn}
\citation{bib:rnn}
\newlabel{eq:err}{{2.5}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Rekurrente Neuronale Netze}{5}}
\citation{bib:rnn}
\citation{bib:vgp}
\citation{bib:vgp}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Um Backpropagation f\IeC {\"u}r ein rekurrentes Netz zu benutzen, muss man es durch die Zeit auffallten. \cite  {bib:rnn}}}{6}}
\newlabel{img:rnn}{{2.4}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Das Vanishing Gradient Problem}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Das Problem vom verschwindendem Gradienten in einem rekurrenten Netz. Ein Fehler im Zeitschritt 1 erzeugt einen Gradienten, der aber auf die folgenden Zeitschritte immer weniger Einfluss hat.\cite  {bib:vgp}}}{7}}
\newlabel{img:vgp}{{2.5}{7}}
\bibcite{bib:neuron}{1}
\bibcite{bib:aneuron}{2}
\bibcite{bib:mlp}{3}
\bibcite{bib:number}{4}
\bibcite{bib:nn}{5}
\bibcite{bib:bp}{6}
\bibcite{bib:rnn}{7}
\bibcite{bib:vgp}{8}
\bibcite{bib:c}{9}
\bibstyle{abbrv}
\citation{bib:neuron}
\citation{bib:aneuron}
\citation{bib:mlp}
\citation{bib:rnn}
\@writefile{toc}{\contentsline {chapter}{Abbildungsverzeichnis}{9}}
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{12.14694pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{19.70898pt}
